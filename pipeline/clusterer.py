import logging
from pathlib import Path
from typing import Tuple, Optional

import numpy as np
import pandas as pd
import config

from sklearn.metrics import silhouette_score

try:
    import hdbscan
except ImportError as e:
    raise ImportError(
        "ğŸ›‘ `hdbscan`ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ``pip install hdbscan``ë¡œ ì„¤ì¹˜ í›„ ì‚¬ìš©í•˜ì„¸ìš”."
    ) from e

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


def cluster_embeddings(
    embeddings_2d: np.ndarray,
    min_cluster_size: int = config.HDBSCAN_MIN_CLUSTER_SIZE,
    min_samples: Optional[int] = None,
    metric: str = config.HDBSCAN_METRIC,
    cluster_selection_epsilon: float = config.HDBSCAN_SELECTION_EPS,
) -> Tuple[np.ndarray, hdbscan.HDBSCAN]:
    """
    Cluster reduced embeddings using HDBSCAN with parameters from config by default.
    - config.HANDLE_OUTLIERS=True ì´ë©´, noise(-1) ë ˆì´ë¸”ì„ config.OUTLIER_LABELë¡œ ì¹˜í™˜í•©ë‹ˆë‹¤.
    """
    if embeddings_2d.ndim != 2:
        raise ValueError("`embeddings_2d` must be 2-D array (n_samples Ã— n_dim).")

    if min_samples is None:
        min_samples = (
            config.HDBSCAN_MIN_SAMPLES
            if hasattr(config, "HDBSCAN_MIN_SAMPLES")
            else max(2, min_cluster_size // 2)
        )

    logger.info(
        "ğŸ§© HDBSCAN clustering: %s â€¢ min_cluster_size=%d â€¢ min_samples=%d â€¢ metric=%s â€¢ epsilon=%.3f",
        embeddings_2d.shape,
        min_cluster_size,
        min_samples,
        metric,
        cluster_selection_epsilon,
    )

    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=min_cluster_size,
        min_samples=min_samples,
        metric=metric,
        cluster_selection_epsilon=cluster_selection_epsilon,
        prediction_data=False,
    ).fit(embeddings_2d)

    labels = clusterer.labels_
    # â”€â”€â”€â”€â”€â”€â”€ outlier ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€
    if getattr(config, "HANDLE_OUTLIERS", False):
        # labels ë°°ì—´ì˜ dtypeì„ objectë¡œ ë°”ê¿”ì•¼ ë¬¸ìì—´ê³¼ ìˆ«ìë¥¼ ì„ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
        labels = np.array(
            [t if t != -1 else config.OUTLIER_LABEL for t in labels],
            dtype=object,
        )
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    n_noise = int((labels == config.OUTLIER_LABEL).sum()) if config.HANDLE_OUTLIERS else int((clusterer.labels_ == -1).sum())

    logger.info(
        "âœ… Clustering complete â€” %d clusters â€¢ %d noise/%s points",
        n_clusters,
        n_noise,
        config.OUTLIER_LABEL if config.HANDLE_OUTLIERS else "-1",
    )
    return labels, clusterer

def evaluate_clusters(
    labels: np.ndarray,
    embeddings_2d: np.ndarray,
    raw_embeddings: np.ndarray | None = None,
    logger: logging.Logger | None = None,
    output_dir: Path | None = None,
    timestamp: str | None = None,
) -> None:
    """
    Log cluster size distribution and silhouette scores.

    Parameters
    ----------
    labels : np.ndarray
        Cluster labels (-1 for noise or other outlier label).
    embeddings_2d : np.ndarray
        UMAPâ€reduced embeddings used for clustering.
    raw_embeddings : np.ndarray, optional
        Original highâ€dimensional embeddings. If provided, will also compute
        silhouette on raw space (cosine).
    logger : logging.Logger, optional
        Logger to use; defaults to module logger.
    output_dir : Path, optional
        Directory to save CSVs.
    timestamp : str, optional
        Timestamp suffix for filenames.
    """
    lg = logger or logging.getLogger(__name__)

    # --- 1) ë¬¸ìì—´ ë ˆì´ë¸” ë³€í™˜ í›„ ë¶„í¬ ê³„ì‚° ---
    # int ì™€ str ì´ ì„ì—¬ ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ ëª¨ë‘ str ë¡œ ë°”ê¿” ì²˜ë¦¬
    labels_str = labels.astype(str)
    unique, counts = np.unique(labels_str, return_counts=True)
    dist_info = ", ".join(f"{c}@{u}" for u, c in zip(unique, counts))
    lg.info("ğŸ”¢ Cluster distribution: %s", dist_info)

    # CSV ë¡œ ì €ì¥
    if output_dir and timestamp:
        df_dist = pd.DataFrame({
            "cluster_label": unique,
            "count": counts,
        })
        dist_path = Path(output_dir) / f"cluster_distribution_{timestamp}.csv"
        df_dist.to_csv(dist_path, index=False, encoding="utf-8-sig")
        lg.info("ğŸ’¾ Cluster distribution saved â†’ %s", dist_path)

    # --- 2) Silhouette ì ìˆ˜ ê³„ì‚° ---
    # noise(-1) ë˜ëŠ” other ì²˜ë¦¬ëœ ë ˆì´ë¸” ì œì™¸
    mask = labels_str != str(config.OUTLIER_LABEL if getattr(config, "OUTLIER_LABEL", "-1") else "-1")
    if mask.sum() >= 2:
        from sklearn.metrics import silhouette_score

        sil_umap = silhouette_score(
            embeddings_2d[mask], labels_str[mask], metric="euclidean"
        )
        msg = f"ğŸ” Silhouette (UMAP): {sil_umap:.3f}"

        if raw_embeddings is not None:
            sil_raw = silhouette_score(
                raw_embeddings[mask], labels_str[mask], metric="cosine"
            )
            msg += f" | (raw): {sil_raw:.3f}"

        lg.info(msg)

        if output_dir and timestamp:
            df_stats = pd.DataFrame([{
                "silhouette_umap": sil_umap,
                "silhouette_raw": sil_raw if raw_embeddings is not None else None,
                "n_clusters": int(len(set(labels_str[mask])) - (1 if str(config.OUTLIER_LABEL) in set(labels_str) else 0)),
                "n_noise": int((~mask).sum()),
            }])
            stats_path = Path(output_dir) / f"cluster_stats_{timestamp}.csv"
            df_stats.to_csv(stats_path, index=False, encoding="utf-8-sig")
            lg.info("ğŸ’¾ Cluster stats saved â†’ %s", stats_path)
    else:
        lg.info("ğŸ” Silhouette: insufficient nonâ€noise points (<2)")
