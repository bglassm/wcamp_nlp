from __future__ import annotations

import argparse
import warnings
import logging
import sys
from pathlib import Path
from typing import List, Dict
from datetime import datetime
import time

import pandas as pd
import numpy as np

import config
from pipeline.loader import load_reviews
from pipeline.preprocess import preprocess_reviews
from pipeline.clause_splitter import split_clauses
from pipeline.absa import classify_clauses
from pipeline.embedder import embed_reviews
from pipeline.reducer import reduce_embeddings
from pipeline.clusterer import cluster_embeddings, evaluate_clusters
from pipeline.tuner import get_cluster_params
from pipeline.summarizer import extract_representatives
from pipeline.mergy import merge_similar_clusters
from pipeline.keywords import extract_keywords
from pipeline.exporter import (
    save_clustered_clauses,
    save_clauses_summary_json,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¡œê¹… & ê²½ê³  ì–µì œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.getLogger("pyabsa").setLevel(logging.ERROR)
logging.getLogger("transformers").setLevel(logging.ERROR)
logging.getLogger("kss").setLevel(logging.ERROR)
logging.getLogger("weasel").setLevel(logging.ERROR)
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=ResourceWarning)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸: ë¼ë²¨ ì˜¤í”„ì…‹/ë³‘í•©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _offset_labels(labels: np.ndarray, base: int) -> np.ndarray:
    """
    HDBSCAN ë¼ë²¨ì„ í´ë¼ë¦¬í‹°ë³„ base(neg=0, neu=1000, pos=2000)ë§Œí¼ ì´ë™.
    -1(ë…¸ì´ì¦ˆ) â†’ base+999 ë¡œ ìˆ«ìží™”.
    """
    out = []
    for l in labels:
        # ë¬¸ìžì—´ 'other' ê°™ì€ ì¼€ì´ìŠ¤ ë°©ì§€
        if isinstance(l, str):
            l = -1 if l.lower() == "other" else int(l)
        if int(l) == -1:
            out.append(base + 999)
        else:
            out.append(int(l) + base)
    return np.array(out, dtype=int)

def _relabel_dict(d: Dict[int, list], base: int) -> Dict[int, list]:
    """
    reps/keywords ë”•ì…”ë„ˆë¦¬ì˜ í‚¤(í´ëŸ¬ìŠ¤í„° id)ë¥¼ baseë§Œí¼ ì´ë™.
    """
    new_d: Dict[int, list] = {}
    for k, v in d.items():
        try:
            kk = int(k)
            new_d[kk + base] = v
        except Exception:
            # í˜¹ì‹œ ë¬¸ìžì—´ 'other' ë“± ë“¤ì–´ì˜¤ë©´ ë…¸ì´ì¦ˆë¡œ ì²˜ë¦¬
            new_d[base + 999] = v
    return new_d

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_full_pipeline(input_files: List[Path], output_dir: Path) -> None:
    total = len(input_files)
    logging.info("â–¶ Starting full pipeline for %d files", total)
    timestamp = datetime.now().strftime("%y%m%d_%H%M%S")

    # í´ë¼ë¦¬í‹° â†’ ì˜¤í”„ì…‹ ë² ì´ìŠ¤
    base_map = {"negative": 0, "neutral": 1000, "positive": 2000}

    for idx, file_path in enumerate(input_files, start=1):
        logging.info("ðŸ”„ [%d/%d] Processing %s", idx, total, file_path.name)
        stem = file_path.stem
        out_dir = output_dir / stem
        out_dir.mkdir(parents=True, exist_ok=True)

        # 1) Load + preprocess
        t0 = time.time()
        logging.info("   1) Loading & preprocessing reviewsâ€¦")
        df = load_reviews(file_path)
        df = preprocess_reviews(df)
        df = df.reset_index(drop=False).rename(columns={"index": config.REVIEW_ID_COL})
        logging.info("      â†’ Loaded %d reviews (%.1fs)", len(df), time.time() - t0)

        # 1.5) Clause splitting
        t0 = time.time()
        logging.info("   1.5) Splitting into clausesâ€¦")
        clause_df = split_clauses(
            df, text_col="review",
            connectives=config.CLAUSE_CONNECTIVES,
            id_col=config.REVIEW_ID_COL,
        )
        logging.info("      â†’ split_clauses done (%d clauses, %.1fs)",
                     len(clause_df), time.time() - t0)

        # 1.6) ABSA
        t0 = time.time()
        logging.info("   1.6) Running ABSA on clauses (batch_size=%d)â€¦",
                     config.ABSA_BATCH_SIZE)
        raw_absa = classify_clauses(
            clause_df,
            model_name=config.ABSA_MODEL_NAME,
            batch_size=config.ABSA_BATCH_SIZE,
            device=config.DEVICE,
        )
        logging.info("      â†’ ABSA complete (%d results, %.1fs)",
                     len(raw_absa), time.time() - t0)

        absa_df = pd.DataFrame(
            raw_absa, columns=["review_id", "clause", "polarity", "confidence"]
        )
        logging.info("      â–¶ [DEBUG] ABSA polarity counts: %s",
                     absa_df["polarity"].value_counts().to_dict())

        # â”€â”€ í†µí•© ì €ìž¥ì„ ìœ„í•œ ëˆ„ì  ë²„í¼ â”€â”€
        combined_clause_df_list = []     # í´ë¼ë¦¬í‹°ë³„ ì ˆ(ë¼ë²¨ ì ìš© í›„) ì´ì–´ë¶™ì´ê¸°
        combined_reps: Dict[int, list] = {}
        combined_kw: Dict[int, list] = {}

        # 1.7) í´ë¼ë¦¬í‹° ë£¨í”„
        for pol in ("negative", "neutral", "positive"):
            sub_df = absa_df[
                (absa_df["polarity"] == pol) &
                (absa_df["confidence"] >= config.ABSA_CONFIDENCE_THRESHOLD)
            ].reset_index(drop=True)
            logging.info("   â–¶ [%s] %d/%d clauses selected (thr=%.2f)",
                         pol, len(sub_df), len(absa_df), config.ABSA_CONFIDENCE_THRESHOLD)
            if sub_df.empty:
                logging.info("      â­ï¸ [%s] no clauses, skip", pol)
                continue

            texts = sub_df["clause"].tolist()

            # 2) Autoâ€tune UMAP/HDBSCAN params
            tuner_params = get_cluster_params(len(texts), dataset=f"{stem}_{pol}")
            umap_p, hdbscan_p = tuner_params["umap"], tuner_params["hdbscan"]

            # 3) Embedding
            emb_t0 = time.time()
            embeddings = embed_reviews(
                texts, model_name=config.MODEL_NAME,
                batch_size=config.BATCH_SIZE, device=config.DEVICE,
            )
            logging.info("      â†’ [%s] Embeddings shape: %s (%.1fs)",
                         pol, embeddings.shape, time.time() - emb_t0)

            # 4) UMAP reduction
            red_t0 = time.time()
            coords = reduce_embeddings(
                embeddings,
                n_components=umap_p["n_components"],
                n_neighbors=umap_p["n_neighbors"],
                min_dist=umap_p["min_dist"],
                metric=umap_p["metric"],
                random_state=umap_p["random_state"],
            )
            logging.info("      â†’ [%s] Reduced coords shape: %s (%.1fs)",
                         pol, coords.shape, time.time() - red_t0)

            # 5) HDBSCAN clustering
            clu_t0 = time.time()
            labels_raw, _ = cluster_embeddings(
                coords,
                min_cluster_size=hdbscan_p["min_cluster_size"],
                min_samples=hdbscan_p["min_samples"],
                metric=hdbscan_p["metric"],
                cluster_selection_epsilon=hdbscan_p["cluster_selection_epsilon"],
            )
            logging.info("      â†’ [%s] Clustered (%d labels) (%.1fs)",
                         pol, len(labels_raw), time.time() - clu_t0)

            # 6) ì§„ë‹¨ (í´ë¼ë¦¬í‹°ë³„ ë¶„í¬/ì‹¤ë£¨ì—£ CSVëŠ” ê·¸ëŒ€ë¡œ ì €ìž¥)
            evaluate_clusters(
                labels_raw, coords, raw_embeddings=embeddings,
                output_dir=out_dir, timestamp=timestamp,
            )

            # 7) ëŒ€í‘œ ë¬¸ìž¥
            reps = extract_representatives(
                texts=texts, embeddings=embeddings,
                labels=labels_raw, top_k=config.TOP_K_REPRESENTATIVES,
            )

            # 8) (ì˜µì…˜) ë³‘í•©
            if getattr(config, "ENABLE_CLUSTER_MERGE", False):
                merge_map, reps, _ = merge_similar_clusters(
                    reps, model_name=config.MODEL_NAME,
                    threshold=getattr(config, "CLUSTER_MERGE_THRESHOLD", 0.90),
                )
                labels_raw = np.array([merge_map.get(str(lbl), lbl) for lbl in labels_raw])

            # 9) í‚¤ì›Œë“œ
            kw = extract_keywords(reps, model_name=config.MODEL_NAME)

            # â”€â”€ ì˜¤í”„ì…‹ ì ìš© í›„ í†µí•© ë²„í¼ì— ì¶•ì  â”€â”€
            base = base_map[pol]
            labels_off = _offset_labels(labels_raw, base)        # np.ndarray(int)
            reps_off = _relabel_dict(reps, base)                 # Dict[int, list]
            kw_off = _relabel_dict(kw, base)                     # Dict[int, list]

            # ì ˆ ë°ì´í„° ëˆ„ì  (ì›ëž˜ sub_df ì»¬ëŸ¼ + ì˜¤í”„ì…‹ ë¼ë²¨ + í´ë¼ë¦¬í‹°)
            combined_clause_df_list.append(
                sub_df.assign(cluster_label=labels_off, polarity=pol)
            )

            # reps/kw ëˆ„ì (í‚¤ ì¶©ëŒ ì—†ìŒ: í´ë¼ë¦¬í‹°ë³„ ë² ì´ìŠ¤ê°€ ë‹¤ë¦„)
            combined_reps.update(reps_off)
            combined_kw.update(kw_off)

        # â”€â”€ í´ë¼ë¦¬í‹° ë£¨í”„ ë: í•œ ë²ˆë§Œ ì €ìž¥ â”€â”€
        if combined_clause_df_list:
            combined_clause_df = pd.concat(combined_clause_df_list, ignore_index=True)

            # Excel (clauses/mapping/reviews)
            save_clustered_clauses(
                clause_df=combined_clause_df,
                raw_df=df,
                keywords=combined_kw,
                output_path=out_dir / f"{stem}_clauses_clustered_{timestamp}.xlsx"
            )

            # Summary JSON (representatives + keywords)
            # JSON í‚¤ëŠ” ë¬¸ìžì—´ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë¤í”„ (ì •ë ¬ì€ íŒŒì¼ ë·°ì–´ì—ì„œ íŽ¸í•¨)
            save_clauses_summary_json(
                combined_clause_df,
                reps=combined_reps,
                kw=combined_kw,
                output_path=out_dir / f"{stem}_clauses_summary_{timestamp}.json"
            )
            logging.info("      ðŸ’¾ merged outputs saved")
        else:
            logging.info("   â­ï¸ No clauses passed threshold for any polarity â€” nothing to save.")

        logging.info("âœ… Completed %s (%d/%d)\n", stem, idx, total)


def main() -> None:
    parser = argparse.ArgumentParser(description="Clause-level clustering pipeline runner")
    parser.add_argument("--files", nargs="*", type=Path, default=config.INPUT_FILES,
                        help="List of input Excel files")
    parser.add_argument("--output_dir", type=Path, default=Path(config.OUTPUT_DIR),
                        help="Directory to save outputs")
    args = parser.parse_args()

    timestamp = datetime.now().strftime("%y%m%d_%H%M%S")
    log_dir = Path(config.OUTPUT_DIR) / "logs"
    log_dir.mkdir(exist_ok=True, parents=True)
    log_path = log_dir / f"clause_pipeline_{timestamp}.log"

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s | %(levelname)s | %(message)s",
        handlers=[logging.FileHandler(log_path, encoding="utf-8"),
                  logging.StreamHandler(sys.stdout)]
    )
    run_full_pipeline(args.files, args.output_dir)


if __name__ == "__main__":
    main()
